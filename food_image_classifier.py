# -*- coding: utf-8 -*-
"""Food Image Classifier

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a-DaSwyTIVpHrugM4-3iD72lN-5X1nEi
"""

# food_image_classifier.py
import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D
from tensorflow.keras.applications import EfficientNetB0, ResNet50V2, MobileNetV3Large
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import requests
from io import BytesIO
import json
import pickle

class FoodImageClassifier:
    def __init__(self, img_size=(224, 224), num_classes=101):
        self.img_size = img_size
        self.num_classes = num_classes
        self.model = None
        self.history = None
        self.class_names = []

        # Food-101 dataset classes
        self.food101_classes = [
            'apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare',
            'beet_salad', 'beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito',
            'bruschetta', 'caesar_salad', 'cannoli', 'caprese_salad', 'carrot_cake',
            'ceviche', 'cheese_plate', 'cheesecake', 'chicken_curry', 'chicken_quesadilla',
            'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder',
            'club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes',
            'deviled_eggs', 'donuts', 'dumplings', 'edamame', 'eggs_benedict',
            'escargots', 'falafel', 'filet_mignon', 'fish_and_chips', 'foie_gras',
            'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari', 'fried_rice',
            'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad', 'grilled_cheese_sandwich',
            'grilled_salmon', 'guacamole', 'gyoza', 'hamburger', 'hot_and_sour_soup',
            'hot_dog', 'huevos_rancheros', 'hummus', 'ice_cream', 'lasagna',
            'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese', 'macarons', 'miso_soup',
            'mussels', 'nachos', 'omelette', 'onion_rings', 'oysters',
            'pad_thai', 'paella', 'pancakes', 'panna_cotta', 'peking_duck',
            'pho', 'pizza', 'pork_chop', 'poutine', 'prime_rib',
            'pulled_pork_sandwich', 'ramen', 'ravioli', 'red_velvet_cake', 'risotto',
            'samosa', 'sashimi', 'scallops', 'seaweed_salad', 'shrimp_and_grits',
            'spaghetti_bolognese', 'spaghetti_carbonara', 'spring_rolls', 'steak', 'strawberry_shortcake',
            'sushi', 'tacos', 'takoyaki', 'tiramisu', 'tuna_tartare',
            'waffles'
        ]

    def create_data_generators(self, train_dir, val_dir=None, test_dir=None):
        """Create data generators with augmentation"""
        # Training data augmentation
        train_datagen = ImageDataGenerator(
            rescale=1./255,
            rotation_range=30,
            width_shift_range=0.3,
            height_shift_range=0.3,
            horizontal_flip=True,
            zoom_range=0.3,
            shear_range=0.2,
            brightness_range=[0.8, 1.2],
            fill_mode='nearest',
            validation_split=0.2 if val_dir is None else 0.0
        )

        # Validation/Test data - only rescaling
        val_datagen = ImageDataGenerator(rescale=1./255)

        # Training generator
        if val_dir is None:
            # Use validation split
            self.train_generator = train_datagen.flow_from_directory(
                train_dir,
                target_size=self.img_size,
                batch_size=32,
                class_mode='categorical',
                subset='training',
                shuffle=True
            )

            self.val_generator = train_datagen.flow_from_directory(
                train_dir,
                target_size=self.img_size,
                batch_size=32,
                class_mode='categorical',
                subset='validation',
                shuffle=False
            )
        else:
            # Separate validation directory
            self.train_generator = train_datagen.flow_from_directory(
                train_dir,
                target_size=self.img_size,
                batch_size=32,
                class_mode='categorical',
                shuffle=True
            )

            self.val_generator = val_datagen.flow_from_directory(
                val_dir,
                target_size=self.img_size,
                batch_size=32,
                class_mode='categorical',
                shuffle=False
            )

        # Test generator (if provided)
        if test_dir:
            self.test_generator = val_datagen.flow_from_directory(
                test_dir,
                target_size=self.img_size,
                batch_size=32,
                class_mode='categorical',
                shuffle=False
            )
        else:
            self.test_generator = None

        # Update class information
        self.num_classes = self.train_generator.num_classes
        self.class_names = list(self.train_generator.class_indices.keys())

        print(f"Found {self.train_generator.samples} training images")
        print(f"Found {self.val_generator.samples} validation images")
        if self.test_generator:
            print(f"Found {self.test_generator.samples} test images")
        print(f"Number of classes: {self.num_classes}")
        print(f"Classes: {self.class_names[:10]}...")  # Show first 10 classes

    def build_transfer_learning_model(self, base_model_name='efficientnet'):
        """Build transfer learning model with pre-trained base"""

        # Choose base model
        if base_model_name.lower() == 'efficientnet':
            base_model = EfficientNetB0(
                weights='imagenet',
                include_top=False,
                input_shape=(*self.img_size, 3)
            )
        elif base_model_name.lower() == 'resnet':
            base_model = ResNet50V2(
                weights='imagenet',
                include_top=False,
                input_shape=(*self.img_size, 3)
            )
        elif base_model_name.lower() == 'mobilenet':
            base_model = MobileNetV3Large(
                weights='imagenet',
                include_top=False,
                input_shape=(*self.img_size, 3)
            )
        else:
            raise ValueError("Supported models: 'efficientnet', 'resnet', 'mobilenet'")

        # Freeze base model initially
        base_model.trainable = False

        # Add custom classification head
        self.model = Sequential([
            base_model,
            GlobalAveragePooling2D(),
            BatchNormalization(),
            Dense(512, activation='relu'),
            Dropout(0.5),
            BatchNormalization(),
            Dense(256, activation='relu'),
            Dropout(0.3),
            Dense(self.num_classes, activation='softmax', name='predictions')
        ])

        # Compile model
        self.model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='categorical_crossentropy',
            metrics=['accuracy', 'top_5_accuracy']
        )

        print(f"Built {base_model_name} transfer learning model")
        print(f"Total parameters: {self.model.count_params():,}")

        return self.model

    def build_custom_cnn(self):
        """Build custom CNN from scratch"""
        self.model = Sequential([
            Conv2D(32, (3, 3), activation='relu', input_shape=(*self.img_size, 3)),
            BatchNormalization(),
            MaxPooling2D(2, 2),

            Conv2D(64, (3, 3), activation='relu'),
            BatchNormalization(),
            MaxPooling2D(2, 2),

            Conv2D(128, (3, 3), activation='relu'),
            BatchNormalization(),
            MaxPooling2D(2, 2),

            Conv2D(256, (3, 3), activation='relu'),
            BatchNormalization(),
            MaxPooling2D(2, 2),

            Conv2D(512, (3, 3), activation='relu'),
            BatchNormalization(),
            MaxPooling2D(2, 2),

            GlobalAveragePooling2D(),
            Dense(1024, activation='relu'),
            Dropout(0.5),
            Dense(512, activation='relu'),
            Dropout(0.3),
            Dense(self.num_classes, activation='softmax')
        ])

        self.model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='categorical_crossentropy',
            metrics=['accuracy', 'top_5_accuracy']
        )

        print("Built custom CNN model")
        return self.model

    def train(self, epochs=50, fine_tune_epochs=20, fine_tune_layers=50):
        """Train model with optional fine-tuning"""
        # Callbacks
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.2,
                patience=5,
                min_lr=1e-7,
                verbose=1
            ),
            ModelCheckpoint(
                'best_food_classifier.h5',
                monitor='val_accuracy',
                save_best_only=True,
                verbose=1
            )
        ]

        # Calculate steps
        steps_per_epoch = self.train_generator.samples // self.train_generator.batch_size
        validation_steps = self.val_generator.samples // self.val_generator.batch_size

        print("=== PHASE 1: Training with frozen base ===")

        # Phase 1: Train with frozen base
        history1 = self.model.fit(
            self.train_generator,
            steps_per_epoch=steps_per_epoch,
            validation_data=self.val_generator,
            validation_steps=validation_steps,
            epochs=epochs,
            callbacks=callbacks,
            verbose=1
        )

        # Phase 2: Fine-tuning (if transfer learning model)
        if fine_tune_epochs > 0 and hasattr(self.model.layers[0], 'trainable'):
            print(f"\n=== PHASE 2: Fine-tuning top {fine_tune_layers} layers ===")

            # Unfreeze top layers of base model
            base_model = self.model.layers[0]
            base_model.trainable = True

            # Freeze bottom layers
            for layer in base_model.layers[:-fine_tune_layers]:
                layer.trainable = False

            # Recompile with lower learning rate
            self.model.compile(
                optimizer=Adam(learning_rate=0.0001),  # Lower learning rate
                loss='categorical_crossentropy',
                metrics=['accuracy', 'top_5_accuracy']
            )

            print(f"Trainable parameters: {sum([tf.keras.backend.count_params(w) for w in self.model.trainable_weights]):,}")

            # Continue training
            history2 = self.model.fit(
                self.train_generator,
                steps_per_epoch=steps_per_epoch,
                validation_data=self.val_generator,
                validation_steps=validation_steps,
                epochs=fine_tune_epochs,
                initial_epoch=len(history1.history['loss']),
                callbacks=callbacks,
                verbose=1
            )

            # Combine histories
            for key in history1.history:
                history1.history[key].extend(history2.history[key])

        self.history = history1
        return history1

    def evaluate(self, generator=None):
        """Evaluate model performance"""
        if generator is None:
            generator = self.test_generator if self.test_generator else self.val_generator

        if generator is None:
            print("No evaluation data available")
            return

        print("Evaluating model...")

        # Get predictions
        steps = generator.samples // generator.batch_size
        predictions = self.model.predict(generator, steps=steps, verbose=1)
        predicted_classes = np.argmax(predictions, axis=1)

        # Get true labels
        generator.reset()
        true_classes = generator.classes[:len(predicted_classes)]

        # Classification report
        report = classification_report(
            true_classes, predicted_classes,
            target_names=self.class_names,
            output_dict=True
        )

        print("\nTop-1 Accuracy:", report['accuracy'])

        # Top-5 accuracy
        top5_accuracy = 0
        for i, pred in enumerate(predictions):
            if true_classes[i] in np.argsort(pred)[-5:]:
                top5_accuracy += 1
        top5_accuracy /= len(predictions)

        print(f"Top-5 Accuracy: {top5_accuracy:.4f}")

        # Confusion matrix for top classes
        top_classes = sorted(self.class_names[:20])  # Show top 20 classes
        top_indices = [self.class_names.index(cls) for cls in top_classes]

        # Filter predictions and true labels for top classes
        mask = np.isin(true_classes, top_indices)
        filtered_true = true_classes[mask]
        filtered_pred = predicted_classes[mask]

        if len(filtered_true) > 0:
            cm = confusion_matrix(filtered_true, filtered_pred, labels=top_indices)

            plt.figure(figsize=(15, 12))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                       xticklabels=top_classes, yticklabels=top_classes)
            plt.title('Confusion Matrix (Top 20 Classes)')
            plt.xlabel('Predicted')
            plt.ylabel('Actual')
            plt.xticks(rotation=45, ha='right')
            plt.yticks(rotation=0)
            plt.tight_layout()
            plt.show()

        return report, top5_accuracy

    def predict_image(self, image_path_or_url, top_k=5):
        """Predict food class for single image"""
        try:
            # Load image
            if image_path_or_url.startswith('http'):
                # Load from URL
                response = requests.get(image_path_or_url)
                image = Image.open(BytesIO(response.content))
            else:
                # Load from file path
                image = load_img(image_path_or_url, target_size=self.img_size)

            # Preprocess
            img_array = img_to_array(image)
            img_array = np.expand_dims(img_array, axis=0)
            img_array /= 255.0

            # Predict
            predictions = self.model.predict(img_array, verbose=0)[0]

            # Get top-k predictions
            top_indices = np.argsort(predictions)[-top_k:][::-1]

            results = []
            for idx in top_indices:
                results.append({
                    'class': self.class_names[idx],
                    'confidence': float(predictions[idx]),
                    'percentage': float(predictions[idx]) * 100
                })

            # Display image with predictions
            plt.figure(figsize=(12, 8))

            plt.subplot(1, 2, 1)
            plt.imshow(image)
            plt.axis('off')
            plt.title('Input Image')

            plt.subplot(1, 2, 2)
            classes = [r['class'].replace('_', ' ').title() for r in results]
            confidences = [r['percentage'] for r in results]

            bars = plt.barh(range(len(classes)), confidences)
            plt.yticks(range(len(classes)), classes)
            plt.xlabel('Confidence (%)')
            plt.title(f'Top {top_k} Predictions')
            plt.gca().invert_yaxis()

            # Color bars by confidence
            for i, bar in enumerate(bars):
                if confidences[i] > 50:
                    bar.set_color('green')
                elif confidences[i] > 20:
                    bar.set_color('orange')
                else:
                    bar.set_color('red')

            plt.tight_layout()
            plt.show()

            return results

        except Exception as e:
            print(f"Error processing image: {e}")
            return None

    def predict_batch(self, image_paths, batch_size=32):
        """Predict multiple images"""
        results = []

        for i in range(0, len(image_paths), batch_size):
            batch_paths = image_paths[i:i+batch_size]
            batch_images = []

            for path in batch_paths:
                try:
                    img = load_img(path, target_size=self.img_size)
                    img_array = img_to_array(img) / 255.0
                    batch_images.append(img_array)
                except:
                    batch_images.append(np.zeros((*self.img_size, 3)))

            batch_images = np.array(batch_images)
            predictions = self.model.predict(batch_images, verbose=0)

            for j, pred in enumerate(predictions):
                top_class = np.argmax(pred)
                confidence = pred[top_class]

                results.append({
                    'image_path': batch_paths[j],
                    'predicted_class': self.class_names[top_class],
                    'confidence': float(confidence)
                })

        return results

    def analyze_model_performance(self):
        """Analyze model performance by class"""
        if self.history is None:
            print("Model hasn't been trained yet")
            return

        # Plot training history
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # Accuracy
        axes[0, 0].plot(self.history.history['accuracy'], label='Training')
        axes[0, 0].plot(self.history.history['val_accuracy'], label='Validation')
        axes[0, 0].set_title('Model Accuracy')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Accuracy')
        axes[0, 0].legend()
        axes[0, 0].grid(True)

        # Loss
        axes[0, 1].plot(self.history.history['loss'], label='Training')
        axes[0, 1].plot(self.history.history['val_loss'], label='Validation')
        axes[0, 1].set_title('Model Loss')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Loss')
        axes[0, 1].legend()
        axes[0, 1].grid(True)

        # Top-5 Accuracy
        if 'top_5_accuracy' in self.history.history:
            axes[1, 0].plot(self.history.history['top_5_accuracy'], label='Training')
            axes[1, 0].plot(self.history.history['val_top_5_accuracy'], label='Validation')
            axes[1, 0].set_title('Top-5 Accuracy')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Top-5 Accuracy')
            axes[1, 0].legend()
            axes[1, 0].grid(True)

        # Learning rate (if available)
        if 'lr' in self.history.history:
            axes[1, 1].plot(self.history.history['lr'])
            axes[1, 1].set_title('Learning Rate')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('Learning Rate')
            axes[1, 1].set_yscale('log')
            axes[1, 1].grid(True)

        plt.tight_layout()
        plt.show()

    def create_food_nutrition_info(self):
        """Create nutrition information lookup for food classes"""
        # Sample nutrition data (calories per 100g)
        nutrition_data = {
            'apple_pie': {'calories': 237, 'category': 'dessert'},
            'pizza': {'calories': 266, 'category': 'main_course'},
            'hamburger': {'calories': 295, 'category': 'main_course'},
            'ice_cream': {'calories': 207, 'category': 'dessert'},
            'french_fries': {'calories': 365, 'category': 'side_dish'},
            'chocolate_cake': {'calories': 371, 'category': 'dessert'},
            'sushi': {'calories': 175, 'category': 'main_course'},
            'salad': {'calories': 65, 'category': 'healthy'},
            'grilled_salmon': {'calories': 231, 'category': 'healthy'},
            # Add more as needed...
        }

        return nutrition_data

    def food_recommendation_system(self, predicted_class, user_preferences=None):
        """Recommend similar foods or healthier alternatives"""
        nutrition_info = self.create_food_nutrition_info()

        recommendations = {
            'similar_foods': [],
            'healthier_alternatives': [],
            'nutrition_info': nutrition_info.get(predicted_class, {})
        }

        # Find similar foods (same category)
        if predicted_class in nutrition_info:
            current_category = nutrition_info[predicted_class]['category']

            for food, info in nutrition_info.items():
                if food != predicted_class and info['category'] == current_category:
                    recommendations['similar_foods'].append(food)

            # Find healthier alternatives (lower calories, healthy category)
            current_calories = nutrition_info[predicted_class]['calories']

            for food, info in nutrition_info.items():
                if (info['calories'] < current_calories * 0.7 or
                    info['category'] == 'healthy'):
                    recommendations['healthier_alternatives'].append(food)

        return recommendations

    def save_model(self, filepath='food_classifier_model.h5'):
        """Save the trained model"""
        if self.model is not None:
            self.model.save(filepath)

            # Save class names
            with open(filepath.replace('.h5', '_classes.json'), 'w') as f:
                json.dump(self.class_names, f)

            print(f"Model saved to {filepath}")
        else:
            print("No model to save")

    def load_model(self, filepath='food_classifier_model.h5'):
        """Load a trained model"""
        try:
            self.model = tf.keras.models.load_model(filepath)

            # Load class names
            with open(filepath.replace('.h5', '_classes.json'), 'r') as f:
                self.class_names = json.load(f)

            self.num_classes = len(self.class_names)
            print(f"Model loaded from {filepath}")
            print(f"Classes: {self.num_classes}")

        except Exception as e:
            print(f"Error loading model: {e}")

def demo_food_classification(classifier, sample_images):
    """Demonstrate food classification with sample images"""
    print("=== FOOD CLASSIFICATION DEMO ===")

    for image_path in sample_images:
        if os.path.exists(image_path):
            print(f"\nAnalyzing: {image_path}")
            results = classifier.predict_image(image_path, top_k=3)

            if results:
                top_prediction = results[0]
                print(f"Predicted: {top_prediction['class'].replace('_', ' ').title()}")
                print(f"Confidence: {top_prediction['percentage']:.1f}%")

                # Get recommendations
                recommendations = classifier.food_recommendation_system(top_prediction['class'])

                if recommendations['nutrition_info']:
                    print(f"Nutrition info: {recommendations['nutrition_info']}")

                if recommendations['healthier_alternatives']:
                    print(f"Healthier alternatives: {recommendations['healthier_alternatives'][:3]}")

def main():
    """Main function demonstrating the food image classification system"""
    # Initialize classifier
    classifier = FoodImageClassifier(img_size=(224, 224))

    # Data directories (adjust paths as needed)
    train_dir = 'food_dataset/train'
    val_dir = 'food_dataset/val'  # Optional
    test_dir = 'food_dataset/test'  # Optional

    if os.path.exists(train_dir):
        print("Setting up data generators...")
        classifier.create_data_generators(train_dir, val_dir, test_dir)

        # Build model
        print("Building transfer learning model...")
        classifier.build_transfer_learning_model('efficientnet')

        # Train model
        print("Training model...")
        history = classifier.train(epochs=30, fine_tune_epochs=10)

        # Analyze performance
        classifier.analyze_model_performance()

        # Evaluate model
        if classifier.test_generator or classifier.val_generator:
            classifier.evaluate()

        # Save model
        classifier.save_model('trained_food_classifier.h5')

        # Demo predictions
        sample_images = ['sample1.jpg', 'sample2.jpg']  # Add actual image paths
        # demo_food_classification(classifier, sample_images)

    else:
        print(f"Dataset directory {train_dir} not found!")
        print("Please organize your dataset as:")
        print("food_dataset/")
        print("  train/")
        print("    pizza/")
        print("      image1.jpg")
        print("    burger/")
        print("      image1.jpg")
        print("    ...")

if __name__ == "__main__":
    main()